# Preference Tuning Research Resources
This is the list of tutorials, workshops, papers, and resources on RL and preference tuning (with or without human feedback) research, which includes most (if not all) papers discussed in our survey paper: [Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey](https://arxiv.org/pdf/2409.11564).

The paper list will be updated over the time. You are always more than welcome to send a pull request for updating the list and be one of the contributors! 

## Table of Contents

- [ðŸš€ Highlights](#-highlights)

- [ðŸ“‘ Research Papers](#-research-papers)
  - [Other Related Survey Papers](#Other-Related-Survey-Papers)
  - [RLHF](#OpenAI-RLHF)

- [Blogs and Links](#Blogs-and-Links)

## ðŸš€ Highlights
- 2025/04/07 We release the third version of our survey paper! We include more related relevant references e.g. [GRPO](https://arxiv.org/abs/2402.03300).
- 2025/02/07 Our paper is accepted by the Journal of Artifical Intelligence Research.

## ðŸ“‘ Research Papers

### Other Related Survey Papers
- <b>opendilab</b> <i>awesome-RLHF</i>. <a href="https://github.com/opendilab/awesome-RLHF">[Github]</a>

### OpenAI RLHF
- <b>Ouyang, et al. (2022)</b> <i>Training Language Models to Follow Instructions with Human Feedback</i>. Arxiv <a href="https://arxiv.org/pdf/2203.02155">[Paper]</a>

## Blogs and Links
- <b>Weng (2024)</b> <i>Reward Hacking</i>. Lilian Weng Blog <a href="https://lilianweng.github.io/posts/2024-11-28-reward-hacking/">[Blog]</a>
